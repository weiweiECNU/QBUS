{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>QBUS6850 - Machine Learning for Business</h1></center>\n",
    "\n",
    "# Tutorial 10 - Neural Networks 1\n",
    "\n",
    "## Keras\n",
    "\n",
    "Keras provides a very high level approach to building, training and predicting using neural networks. It is built on top of other neural network backend libraries like TensorFlow or Theano. It helps to prototype neural networks very quickly.\n",
    "\n",
    "https://keras.io\n",
    "\n",
    "### Installing Keras\n",
    "\n",
    "For this tutorial you will need to install the Keras library:\n",
    "- **Lab Computers**: If you are using the lab computers you can get the qbus6850 environment from GRASP, it already has keras installed.\n",
    "- **Personal Computer**: If you want to install Keras on your laptop etc you can type the following on the Windows Command Prompt or Terminal on OS X:\n",
    "      \n",
    "      conda install –c conda-forge keras\n",
    "\n",
    "### Keras Error (No module named...)\n",
    "\n",
    "Occasionally you may run into one of the following errors:\n",
    "\n",
    "    No module named tensorflow\n",
    "    \n",
    "    No module named theano\n",
    "\n",
    "This is because keras configuration states that tensorflow is installed when theano was installed and vice versa.\n",
    "\n",
    "You need to make sure the keras JSON configuration file uses the backend that was installed. The configuration file is located in either:\n",
    "\n",
    "    /Users/YOUR_USERNAME/.keras/keras.json\n",
    "\n",
    "    C:\\Users\\YOUR_USERNAME\\.keras\\keras.json\n",
    "\n",
    "\n",
    "An example of the config file is below. You just need to change the backend to \"tensorflow\" or \"theano\" depending on whichever was actually installed.\n",
    "\n",
    "    {\n",
    "        \"floatx\": \"float32\",\n",
    "        \"epsilon\": 1e-07,\n",
    "        \"backend\": \"tensorflow\",\n",
    "        \"image_data_format\": \"channels_last\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Neural Network\n",
    "\n",
    "First we will look at building a simple network to perform linear regression.\n",
    "\n",
    "The design of the network dictates the type of function it is performing. So we must carefully match the design with that of a linear regression.\n",
    "\n",
    "We can visualise this as a neural network:\n",
    "\n",
    "<img src=\"nnet1.png\" style=\"width: 300px;\"/>\n",
    "\n",
    "*image from http://dungba.org/linear-regression-with-neural-network/*\n",
    "\n",
    "**Note that in this simple case there are no hidden layers in the network.**\n",
    "\n",
    "Let's begin by loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.layers.core import Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "df = pd.read_csv(\"Advertising.csv\", index_col=0)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually with Neural Networks we scale all features to be in the range (0-1) to reduce the effect of feature domination and gradient vanishing. This particularly helps for deep networks. So it is a good habit to get into.\n",
    "\n",
    "Don't worry we can easily transform the data back into the original range later with the scaler object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "data = scaler.fit_transform(df.values)\n",
    "\n",
    "df_scaled = pd.DataFrame(data, columns=df.columns)\n",
    "\n",
    "df_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can do a train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = df_scaled[\"Sales\"]\n",
    "X = df_scaled[ df_scaled.columns.difference([\"Sales\"]) ]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Neural Network\n",
    "\n",
    "There are three stages to building neural networks with Keras:\n",
    "- defining\n",
    "- compiling\n",
    "- training\n",
    "\n",
    "When we define a network we specify the layers, their dimensions and any activation functions. Compiling instructs Keras on how it should learn the weights by defining the loss metric and optimisation algorithm. Training performs backpropogation to actually learn the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the network architecture\n",
    "model = Sequential()\n",
    "\n",
    "n_features = X_train.shape[1]\n",
    "\n",
    "# First layer, input size is the number of input features and output size is 1\n",
    "# We are doing a linear regression, and let's use linearactivation\n",
    "model.add(Dense(1, input_dim=n_features, activation='linear', use_bias=True))\n",
    "\n",
    "# Use mean squared error for the loss metric and use the ADAM backprop algorithm\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# Train the network (learn the weights)\n",
    "# We need to convert from DataFrame to NumpyArray\n",
    "history = model.fit(X_train.values, y_train.values,  epochs=100, \n",
    "                    batch_size=1, verbose=2, validation_split=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising a Network\n",
    "\n",
    "Often our network can get quite complicated and a visualisation can help with understanding what is happening at each layer.\n",
    "\n",
    "Below is an example of how to plot a model as a Figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils.vis_utils import model_to_dot\n",
    "from IPython.display import Image, display\n",
    "\n",
    "dot_obj = model_to_dot(model, show_shapes = True, show_layer_names = True)\n",
    "\n",
    "display(Image(dot_obj.create_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also save your figure to an PDF or image file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "\n",
    "plot_model(model, to_file='model.pdf', show_shapes = True, show_layer_names = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Inspection\n",
    "\n",
    "You can present a tabular summary of your neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view the weights for every layer you can use a loop. Since we only have a single layer we only have one array of weights plus the bias term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(model.layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    weights = layer.get_weights()\n",
    "    print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison to OLS\n",
    "\n",
    "Now lets compare with the parameters estimated by sklearn's OLS function. The parameters are very close!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "ols = LinearRegression(fit_intercept=True)\n",
    "ols.fit(X_train, y_train)\n",
    "\n",
    "ols_coefs = np.concatenate( ([ols.intercept_], ols.coef_), axis = 0 )\n",
    "print(\"OLS Coefs: {0}\".format(ols_coefs))\n",
    "\n",
    "weights = model.layers[0].get_weights()\n",
    "\n",
    "nn_coefs = np.concatenate( (weights[1], weights[0][:,0]) , axis = 0 )\n",
    "print(\"NN Coefs: {0}\".format(nn_coefs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classifier Network\n",
    "\n",
    "To build a binary classifier network we only have one requirement: that the output is the probability of the data belonging to the class or a class label.\n",
    "\n",
    "To achieve this for a binary classifier we just need a single neuron on the output layer. Then we need to apply an activation function to limit the output to the range [0, 1].\n",
    "\n",
    "We can achieve this with the sigmoid activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# Load the data\n",
    "bank_df = pd.read_csv(\"bank.csv\")\n",
    "\n",
    "X = bank_df.iloc[:, 0:-1]\n",
    "y = bank_df['y_yes']\n",
    "\n",
    "# Scale the data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X.values), columns=X.columns)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y)\n",
    "\n",
    "n_features = X_train.shape[1]\n",
    "\n",
    "# Build our classifier\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=n_features, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "# Final output layer is a single neuron with sigmoid activation\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "# Use mean squared error for the loss metric and use the ADAM backprop algorithm\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "# Train the network (learn the weights)\n",
    "# We need to convert from DataFrame to NumpyArray\n",
    "history = model.fit(X_train.values, y_train.values,  epochs=20, batch_size=16, verbose=1, \n",
    "                                       validation_split=0, class_weight={0:0.0001, 1:100})\n",
    "y_pred = model.predict(X_test.values)\n",
    "\n",
    "print(classification_report(y_test, y_pred.astype(int)))\n",
    "print(confusion_matrix(y_test, y_pred.astype(int)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have manually set the class weights in the previous example since we have a very imbalanced dataset. If you want a good starting point for class weights you can use sklearn to help, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "class_weight = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Class Classification Network\n",
    "\n",
    "In the multi class classifer case we still need class probabilities but we need a class probability for all classes and we need the sum of all probabilties to equal 1.\n",
    "\n",
    "We can do this using the softmax activation function!\n",
    "\n",
    "However this means we need to adjust the shape of our target variable to match the shape of the softmax output, which is a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD\n",
    "np.random.seed(0)\n",
    "\n",
    "wine_df = pd.read_csv('winequality-white.csv', delimiter=\";\")\n",
    "\n",
    "X = wine_df.iloc[:, :-1]\n",
    "y = wine_df.iloc[:, -1]\n",
    "\n",
    "# Convert to dummies to get same shape as softmax output\n",
    "y = pd.get_dummies(y)\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X.values), columns=X.columns)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, random_state=1)\n",
    "\n",
    "n_features = X_train.shape[1]\n",
    "\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_dim=n_features))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "# Softmax output layer\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train.values, y_train.values, epochs=20, batch_size=16)\n",
    "\n",
    "y_pred = model.predict(X_test.values)\n",
    "\n",
    "y_te = np.argmax(y_test.values, axis = 1)\n",
    "y_pr = np.argmax(y_pred, axis = 1)\n",
    "\n",
    "print(np.unique(y_pr))\n",
    "\n",
    "print(classification_report(y_te, y_pr))\n",
    "\n",
    "print(confusion_matrix(y_te, y_pr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Parameters\n",
    "\n",
    "Sometimes tuning for your data can take a lot of time. It is part of the skill of using neural networks. Please familiarize yourself with the terminology and how each parameter affects fitting result.\n",
    "\n",
    "Batch size is the number of samples used in each forward/backward pass of the network. You will notice the number of samples in each Epoch increasing by the batch size. It is a good idea to keep your batch size as a power of 2 e.g. 8, 16, 32, 64, 128 for speed reasons.\n",
    "\n",
    "An Epoch is a forward/backward pass of all batches. If you increase the number of Epoch you should see an improvement in accuracy since it is refining the model parameters each time.\n",
    "\n",
    "The validation split is the proportion of data held out for use as validation set. It provides you with an estimate of the error on the test set. But it is not used for any actual cross validation or optimisation of parameters. For small amounts of data and for low epoch’s it is better to keep this number small or 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation - Estimating Performance\n",
    "\n",
    "Keras is not tightly integrated with scikit learn. So to use normal sklearn functions like cross_val_score we need to wrap Keras models with a class that behaves like an sklearn class. Or we can take a more manual approach.\n",
    "\n",
    "### Method 1 - Keras Wrappers\n",
    "\n",
    "We can use cross_val_score through the use Keras' wrapper function KerasClassifier()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', input_dim=n_features))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "sk_model = KerasClassifier(build_fn=create_model, epochs=20, batch_size=16, verbose=0)\n",
    "\n",
    "print(\"Running...\")\n",
    "cv_scores = cross_val_score(sk_model, X_train.values, y_train.values, scoring = 'neg_log_loss')\n",
    "\n",
    "print(cv_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2 - Manual K-Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "n_features = X_train.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_dim=n_features))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "kf = KFold(3, shuffle=False)\n",
    "\n",
    "cv_scores = list()\n",
    "\n",
    "print(\"Running...\")\n",
    "for train_index, val_index in kf.split(X_train.T):\n",
    "    X_cvtrain, X_cvval = X_train.iloc[train_index, :], X_train.iloc[val_index, :]\n",
    "    y_cvtrain, y_cvval = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "    model.fit(X_cvtrain.values, y_cvtrain.values, epochs=20, batch_size=16, verbose=0)\n",
    "    \n",
    "    score = model.evaluate(X_cvval.values, y_cvval.values, verbose=0)\n",
    "    \n",
    "    # First item in scores is the loss. We specified the loss as crossentropy so we take the negative of it.\n",
    "    cv_scores.append(-score)\n",
    "    \n",
    "print(cv_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation - Hyper Parameter Optimisation\n",
    "\n",
    "### Method 1 - GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def create_model(optimizer='rmsprop'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', input_dim=n_features))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(7, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "optimizers = ['rmsprop']\n",
    "epochs = [5, 10, 15]\n",
    "batches = [128]\n",
    "\n",
    "\n",
    "param_grid = dict(optimizer=optimizers, epochs=epochs, batch_size=batches, verbose=['2'])\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid)\n",
    "\n",
    "grid.fit(X_train.values, y_train.values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation - Optimising Layers\n",
    "\n",
    "You can also combine GridSearchCV and the method below to optimise the hyperparameters and layers at the same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_layers = [  \n",
    "            Dense(64, activation='relu', input_dim=n_features),\n",
    "            Dropout(0.5),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dropout(0.5)\n",
    "         ]\n",
    "\n",
    "final_layer = Dense(7, activation='softmax')\n",
    "\n",
    "class model_builder:\n",
    "    \n",
    "    def __init__(self, layers, final_layer):\n",
    "        self.layers = layers\n",
    "        self.final_layer = final_layer\n",
    "        \n",
    "    def __call__(self):\n",
    "        self.model = Sequential()\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            self.model.add(layer)\n",
    "\n",
    "        self.model.add(self.final_layer)\n",
    "\n",
    "        self.model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        \n",
    "        return self.model\n",
    "\n",
    "    \n",
    "cv_mean_scores = list()\n",
    "print(\"Running...\")\n",
    "for i in range(1, len(total_layers)):\n",
    "    \n",
    "    layers = total_layers[:i]\n",
    "    \n",
    "    mdl_builder = model_builder(layers, final_layer)\n",
    "    \n",
    "    sk_model = KerasClassifier(build_fn=mdl_builder, epochs=20, batch_size=16, verbose=1)\n",
    "    \n",
    "    cv_scores = cross_val_score(sk_model, X_train.values, y_train.values, scoring = 'neg_log_loss')\n",
    "    \n",
    "    cv_mean_scores.append(np.mean(cv_scores))\n",
    "\n",
    "# Put a new line in and then print out mean cv scores\n",
    "# Each score is the CV score for a network with different layers\n",
    "print(\"\\n\")\n",
    "print(cv_mean_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
