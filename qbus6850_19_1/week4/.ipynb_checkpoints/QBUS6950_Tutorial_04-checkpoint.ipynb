{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>QBUS6850 - Machine Learning for Business</h1></center>\n",
    "\n",
    "# Tutorial 4 - Linear Regression 2, Feature Extraction/Prepraration, Logistic Regression\n",
    "\n",
    "## Ordinary Least Squares Drawbacks\n",
    "\n",
    "$$min_{\\boldsymbol  \\beta} \\frac{1}{2N}\\| \\mathbf{t-X} \\boldsymbol \\beta \\|^2$$\n",
    "\n",
    "$$\\boldsymbol \\beta =  (\\mathbf X^T \\mathbf X)^{-1} \\mathbf X^T \\mathbf t$$\n",
    "\n",
    "OLS (Oridnary least squares) is an unbiased estimator by definition,\n",
    "however this does not mean that it always produces a good model estimation.\n",
    "\n",
    "OLS can completely fail or produce poor results under relatively\n",
    "common conditions.\n",
    "\n",
    "The shared point of failure among all these conditions is that\n",
    "($\\mathbf X^T \\mathbf X$) is singular and OLS estimates depend on $(\\mathbf X^T \\mathbf X)^{-1}$. When $(\\mathbf X^T \\mathbf X)$ is\n",
    "singular we cannot compute its inverse.\n",
    "\n",
    "This happens frequently due to:\n",
    "- collinearity i.e. predictors (features) are not independant\n",
    "- $d > N$ i.e. number of features exceeds number of observations\n",
    "\n",
    "Let $\\mathbf M = \\mathbf X^T \\mathbf X \\in \\mathbb R^{d \\times d}$ then $M_{ij} = \\langle X_{(:, i)}, X_{(:, j)} \\rangle$.\n",
    "\n",
    "Or in other words each element of $\\mathbf M$ encodes the similarity or distance from each feature vector to every other feature vector. When we have colinear features this can result in $rank(\\mathbf M) < d$ meaning that $\\mathbf M$ is singular and we cannot compute its inverse.\n",
    "\n",
    "** Moreover OLS may be subject to outliers. This means that a single large outlier value can greatly affect our regression line. Therefore we know OLS is low (0) bias but high variance.  We might wish to\n",
    "sacrifice some bias to achieve lower variance.**\n",
    "\n",
    "\n",
    "## Ridge Regression\n",
    "\n",
    "Ridge regression addresses both issues of high variance and invertibility.\n",
    "\n",
    "By reducing the magnitude of the coefficients by using the $\\ell_2$ norm we can reduce the coefficient variance and thus the variance of the estimator. Therefore the ridge regression objective function is\n",
    "\n",
    "$$min_{\\boldsymbol \\beta} \\frac{1}{2N}\\| \\mathbf{t - X} \\boldsymbol \\beta \\|^2 + \\frac{\\lambda}{2N} \\|\\boldsymbol \\beta \\|^2$$\n",
    "\n",
    "where $\\lambda$ is a tunable parameter and controls the strength of the penalty. Therefore the solution is given by\n",
    "\n",
    "$$\\boldsymbol \\beta =  (\\mathbf X^T \\mathbf X + \\lambda \\mathbf I)^{-1} \\mathbf X^T \\mathbf t$$\n",
    "\n",
    "By reducing the variance of the estimator, ridge regression tends to be more \"robust\" against outliers than OLS since we have a solution with greater bias and less variance. Conventiantly this means that $\\mathbf X^T \\mathbf X$ is then modified to $\\mathbf X^T \\mathbf X + \\lambda \\mathbf I$. In other words we add a diagonal component to the matrix that we want to invert. This diagonal component is called the \"ridge\". This diagonal component increases the rank so that the matrix is no longer singular.\n",
    "\n",
    "We can use cross validation to find the shrinkage parameter that we believe will generalise best to unseen data.\n",
    "\n",
    "**About the Data**\n",
    "\n",
    "The dataset is a collection of community and crime statistics from #http://archive.ics.uci.edu/ml/datasets/communities+and+crime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, RidgeCV\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#http://archive.ics.uci.edu/ml/datasets/communities+and+crime\n",
    "\n",
    "crime = pd.read_csv('communities.csv', header=None, na_values=['?'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-processing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Delete the first 5 columns\n",
    "crime = crime.iloc[:, 5:]\n",
    "\n",
    "# Remove rows with missing entries\n",
    "crime.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract all the features available as our predictors and the response variable (number of violent crimes per capita)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the features X and target/response y\n",
    "X = crime.iloc[:, :-1]\n",
    "t = crime.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate model performance we will split the data into train and test sets.\n",
    "\n",
    "Our procedure will follow:\n",
    "- Fit model on training set\n",
    "- Test performance via loss function values on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split data into train and test sets\n",
    "X_train, X_test, t_train, t_test = train_test_split(X, t, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit OLS model and calculate loss values over test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLS: 0.02733441765802363\n"
     ]
    }
   ],
   "source": [
    "lm = LinearRegression()\n",
    "\n",
    "lm.fit(X_train, t_train)\n",
    "\n",
    "preds_ols = lm.predict(X_test)\n",
    "\n",
    "print(\"OLS: {0}\".format(mean_squared_error(t_test, preds_ols)/2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate possible candidate sets for $\\lambda$, fit the ridge model using cross validation on training set to find optimal $\\lambda$ and calculate mse over test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RIDGE: 0.013305662943587152\n"
     ]
    }
   ],
   "source": [
    "alpha_range = 10.**np.arange(-2,3)\n",
    "\n",
    "rregcv = RidgeCV(normalize=True, scoring='neg_mean_squared_error', alphas=alpha_range)\n",
    "\n",
    "rregcv.fit(X_train, t_train)\n",
    "\n",
    "preds_ridge = rregcv.predict(X_test)\n",
    "print(\"RIDGE: {0}\".format(mean_squared_error(t_test, preds_ridge)/2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However RidgeCV can also be used without those parameters eg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RIDGE: 0.012946592772763638\n"
     ]
    }
   ],
   "source": [
    "rregcv = RidgeCV()\n",
    "\n",
    "rregcv.fit(X_train, t_train)\n",
    "\n",
    "preds_ridge = rregcv.predict(X_test)\n",
    "print(\"RIDGE: {0}\".format(mean_squared_error(t_test, preds_ridge)/2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Feature Extraction\n",
    "- Bag of words\n",
    "- Embedding\n",
    "\n",
    "### Bag of Word\n",
    "    Bag of words(1-gram) counts the words and keep the numbers and serve as the features.\n",
    "    \n",
    "    3 steps needs to be performed to get Bag of Word(BOW) features:\n",
    "    1. Tokenizing: Segement the corpus into \"words\"\n",
    "    2. Counting: Count the appearance frequecy of difference words.\n",
    "    3. Mormalizing\n",
    "    \n",
    "    CountVectorizer from sklearn combine the tokenizing and counting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 1 0 1 0 2 1 0 1]\n",
      " [1 0 0 0 1 0 1 1 0]\n",
      " [0 1 1 1 0 0 1 0 1]]\n",
      "{'this': 8, 'is': 3, 'the': 6, 'first': 2, 'document': 1, 'second': 5, 'and': 0, 'third': 7, 'one': 4}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?',\n",
    "    ]\n",
    "\n",
    "X_txt = vectorizer.fit_transform(corpus)\n",
    "vectorizer.get_feature_names() == (\n",
    "        ['and', 'document', 'first', 'is', 'one',\n",
    "         'second', 'the', 'third', 'this'])\n",
    "\n",
    "#X is the BOW feature of X\n",
    "print(X_txt.toarray())\n",
    "\n",
    "#the value is vocab id\n",
    "print(vectorizer.vocabulary_)\n",
    "\n",
    "#Unseen words are ignore\n",
    "vectorizer.transform(['Something completely new.']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of Words features cannot caputer local information.\n",
    "E.g. \"believe or not\" has the same features as \"not or believe\".\n",
    "Bi-gram preserve more local information, which regrads 2 contagious words as one word in the vocabulary.\n",
    "In the example, \"believe or\", \"or not\", \"not or\" and \"or believe\" are counted.\n",
    "The feature is shown in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['believe or', 'or not', 'not a', 'a b', 'b c', 'c d', 'd e']\n"
     ]
    }
   ],
   "source": [
    "bigram_vectorizer = CountVectorizer(ngram_range=(2, 2),\n",
    "                                        token_pattern=r'\\b\\w+\\b', min_df=1)\n",
    "analyze = bigram_vectorizer.build_analyzer()\n",
    "print(analyze('believe or not a b c d e'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 0 0 0 1 0 0 0 1 0]\n",
      " [0 0 1 0 1 1 0 1 0 0 1 0]\n",
      " [1 0 0 0 0 0 0 0 1 1 0 0]\n",
      " [0 1 0 1 0 0 1 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "X_txt_2 = bigram_vectorizer.fit_transform(corpus).toarray()\n",
    "print(X_txt_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf-idf\n",
    "Some words has very high frequency(e.g. “the”, “a”, ”which”), therefore, carrying not much meaningful information about the actual contents of the document.\n",
    "\n",
    "We need to compensate them to prevent the high-frequency shadowing other words.\n",
    "\n",
    "$td-idf(t, d) = tf(t, d) \\times idf(t)$\n",
    "\n",
    "$idf(t) = log(\\frac{1 + n_d}{1 + df(d, t)}) + 1$ (why +1 ?)\n",
    "$n_d$ is the number of document.\n",
    "$df(t)$ is the number of documents containing $t$.\n",
    "\n",
    "Each row is normalized to have unit Euclidean norm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2)\t0.5732079309279059\n",
      "  (0, 0)\t0.8194099510753754\n",
      "  (1, 0)\t1.0\n",
      "  (2, 0)\t1.0\n",
      "  (3, 0)\t1.0\n",
      "  (4, 1)\t0.8808994832762984\n",
      "  (4, 0)\t0.47330339145578754\n",
      "  (5, 2)\t0.8135516873095774\n",
      "  (5, 0)\t0.5814926070688599\n",
      "[[0.81940995 0.         0.57320793]\n",
      " [1.         0.         0.        ]\n",
      " [1.         0.         0.        ]\n",
      " [1.         0.         0.        ]\n",
      " [0.47330339 0.88089948 0.        ]\n",
      " [0.58149261 0.         0.81355169]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer = TfidfTransformer(smooth_idf=False)\n",
    "counts = [[3, 0, 1],\n",
    "            [2, 0, 0],\n",
    "            [3, 0, 0],\n",
    "            [4, 0, 0],\n",
    "            [3, 2, 0],\n",
    "            [3, 0, 2]\n",
    "         ]\n",
    "tfidf = transformer.fit_transform(counts)\n",
    "print(tfidf)\n",
    "print(tfidf.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A even more concise way to compute the tf-idf features. Combine counts and tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is the first document.', 'This is the second second document.', 'And the third one.', 'Is this the first document?']\n",
      "[[0.         0.43877674 0.54197657 0.43877674 0.         0.\n",
      "  0.35872874 0.         0.43877674]\n",
      " [0.         0.27230147 0.         0.27230147 0.         0.85322574\n",
      "  0.22262429 0.         0.27230147]\n",
      " [0.55280532 0.         0.         0.         0.55280532 0.\n",
      "  0.28847675 0.55280532 0.        ]\n",
      " [0.         0.43877674 0.54197657 0.43877674 0.         0.\n",
      "  0.35872874 0.         0.43877674]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "print(corpus)\n",
    "X_txt_3 = vectorizer.fit_transform(corpus)\n",
    "print(X_txt_3.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "- LASSO\n",
    "- Elastic-Net\n",
    "- Refer to lecture slides for their loss functions\n",
    "\n",
    "We can take two approaches when using LASSO or Elastic-net for feature\n",
    "selection. We can softly regularise the coefficients until a sufficient number\n",
    "are set to 0. Fortunately using cross validation to optimise the regularisation\n",
    "parameter lambda (called alpha in sklearn) usually results in many of the\n",
    "features being ignored since their coefficient values are shrunk to 0.\n",
    "\n",
    "Alternatively you can set a threshold value for the coefficient and find\n",
    "a suitable regularisation parameter that meets this requirement.\n",
    "\n",
    "We will take the path of cross validation.\n",
    "\n",
    "Note that due to the shared L1/L2 regularisation of Elastic-Net it does not\n",
    "aggressively prune features like Lasso. However in practice it often performs\n",
    "better when used for regression prediction.\n",
    "\n",
    "**NOTE: You can also use LASSO/Elastic-Net for regular regression tasks.**\n",
    "\n",
    "Fit the LASSO model. Performed a train/test split and used CV on the train set to determine optimal parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jkan/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/Users/jkan/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/jkan/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/jkan/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/jkan/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/jkan/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/jkan/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/jkan/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LASSO: 0.01298945881444861\n",
      "LASSO Lambda: 0.0031688010361581157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jkan/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/jkan/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/jkan/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/jkan/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/jkan/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/jkan/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/jkan/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/jkan/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/jkan/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/jkan/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/jkan/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/jkan/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/jkan/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LassoCV, ElasticNetCV\n",
    "\n",
    "X = crime.iloc[:, :-1]\n",
    "t = crime.iloc[:, -1]\n",
    "X_train, X_test, t_train, t_test = train_test_split(X, t, random_state=1)\n",
    "\n",
    "lascv = LassoCV()\n",
    "lascv.fit(X_train, t_train)\n",
    "\n",
    "preds_lassocv = lascv.predict(X_test)\n",
    "print(\"LASSO: {0}\".format(mean_squared_error(t_test, preds_lassocv)/2))\n",
    "print(\"LASSO Lambda: {0}\".format(lascv.alpha_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the Elastic-Net model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jkan/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elastic Net: 0.012887092542898677\n",
      "Elastic Net Lambda: 0.006337602072316219\n"
     ]
    }
   ],
   "source": [
    "elascv = ElasticNetCV(max_iter=10000)\n",
    "elascv.fit(X_train, t_train)\n",
    "\n",
    "preds_elascv = elascv.predict(X_test)\n",
    "print(\"Elastic Net: {0}\".format(mean_squared_error(t_test, preds_elascv)/2))\n",
    "print(\"Elastic Net Lambda: {0}\".format(elascv.alpha_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine which columns were retained by each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LASSO Features: [  7   8  20  22  33  37  39  43  49  55  73  76  95 107 113 121 124 125]\n",
      "Elastic Features: [  7   8  20  22  33  37  39  43  46  48  49  50  55  73  76  82  95 107\n",
      " 113 121 124 125]\n"
     ]
    }
   ],
   "source": [
    "columns = X.columns.values\n",
    "\n",
    "lasso_cols = columns[np.nonzero(lascv.coef_)]\n",
    "print(\"LASSO Features: {0}\".format(lasso_cols))\n",
    "\n",
    "elas_cols = columns[np.nonzero(elascv.coef_)]\n",
    "print(\"Elastic Features: {0}\".format(elas_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression vs Classification\n",
    "\n",
    "- Logistic Regression\n",
    "\n",
    "**WARNING: Do not use regression for classification tasks.**\n",
    "\n",
    "In general it is ill advised to use linear regression for\n",
    "classification tasks. Regression learns a continious output variable from\n",
    "a predefined linear (or higher order) model. It learns the parameters\n",
    "of this model to predict an output.\n",
    "\n",
    "Classification on the other hand is not explicity interested in the\n",
    "underlying generative process. Rather it is a higher abstraction. We are not\n",
    "interested in the specific value of something. Instead we want to assign\n",
    "each data vector to the most likely class.\n",
    "\n",
    "Logistic regression provides us with two desirable properties:\n",
    "- the output of the logistic function is the direct probability of the data vector belonging to the success case\n",
    "\n",
    "$f(x, \\beta) = \\frac {1}{1+e^{-(\\beta_0 + \\beta_1 x)}}$\n",
    "\n",
    "- the logistic function is non-linear and more flexible than a linear regression, which can improve classification accuracy and is often more robust to outliers. \n",
    "\n",
    "**About the dataset**\n",
    "\n",
    "The data shows credit card loan status for many accounts with three features: student, balance remaining and income."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "df = pd.read_csv('Default.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the student category column to Boolean values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.student = np.where(df.student == 'Yes', 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the balance feature and set the default status as the target value to predict. You could also use all available features if you believe that they are informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = df[['balance']]\n",
    "t = df[['default']]\n",
    "\n",
    "X_train, X_val, t_train, t_val = train_test_split(X, t, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jkan/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/jkan/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_res = LogisticRegression()\n",
    "\n",
    "log_res.fit(X_train, t_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict probabilities of default.\n",
    "\n",
    "predict_proba() returns the probabilities of an observation belonging to each class. This is computed from the logistic regression function (see above).\n",
    "\n",
    "predict() is dependant on predict_proba(). predict() returns the class assignment based on the proability and the decision boundary. In other words predict returns the most likely class i.e. the class with greatest probability or probability > 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.96903859 0.03096141]\n",
      " [0.10491157 0.89508843]]\n",
      "Probability of default with Balance of 1200: 3.10%\n",
      "Probability of default with Balance of 2500: 89.51%\n",
      "Assigned class with Balance of 1200: 0\n",
      "Assigned class with Balance of 2500: 1\n"
     ]
    }
   ],
   "source": [
    "prob = log_res.predict_proba(pd.DataFrame({'balance': [1200, 2500]}))\n",
    "print(prob)\n",
    "print(\"Probability of default with Balance of 1200: {0:.2f}%\".format(prob[0,1] * 100))\n",
    "print(\"Probability of default with Balance of 2500: {0:.2f}%\".format(prob[1,1] * 100))\n",
    "\n",
    "outcome = log_res.predict(pd.DataFrame({'balance': [1200, 2500]}))\n",
    "print(\"Assigned class with Balance of 1200: {0}\".format(outcome[0]))\n",
    "print(\"Assigned class with Balance of 2500: {0}\".format(outcome[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate classification accuracy using confusion matrix (to be explanined in week 4 lecture) and the classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2903    6]\n",
      " [  66   25]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "pred_log = log_res.predict(X_val)\n",
    "\n",
    "print(confusion_matrix(t_val,pred_log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.978     0.998     0.988      2909\n",
      "           1      0.806     0.275     0.410        91\n",
      "\n",
      "   micro avg      0.976     0.976     0.976      3000\n",
      "   macro avg      0.892     0.636     0.699      3000\n",
      "weighted avg      0.973     0.976     0.970      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report  \n",
    "\n",
    "print(classification_report(t_val, pred_log, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
