{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>QBUS6850 - Machine Learning for Business</h1></center>\n",
    "\n",
    "# Tutorial 8 - Advanced Classification Techniques II\n",
    "\n",
    "## Random Forest Classification\n",
    "\n",
    "In sklearn tree's in a forest are built by bootstrapping the training set. So each tree is built from a slightly different set of data.\n",
    "\n",
    "Sklearn also uses a random subset of features when deciding splits. This is to decrease the variance of the forest by introducing some randomness at the cost of increasing bias. The idea is that we will achieve a good spread of features used. If we were to split each tree using the same features then they would end up identical! Overall this strategy yields a better model.\n",
    "\n",
    "The final classification of each class is given by averaging the probability output of each tree. In other words we caclulate predict_proba() from each tree and average them together. Then pick the most likely class.\n",
    "\n",
    "### Bank Customer Example\n",
    "\n",
    "Lets use a random forest to classify customers in the bank customer dataset.\n",
    "\n",
    "First load the data and build the train/test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "\n",
    "bank_df = pd.read_csv(\"bank.csv\")\n",
    "\n",
    "X = bank_df.iloc[:, 0:-1]\n",
    "y = bank_df['y_yes']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a Random Forest in sklearn is just like using any other classifier. Create the object and then call the object's fit() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
       "            criterion='gini', max_depth=None, max_features='auto',\n",
       "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "            min_impurity_split=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = ensemble.RandomForestClassifier(class_weight = 'balanced')\n",
    "\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let's check the classification accuracy on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.98      0.94       993\n",
      "          1       0.62      0.18      0.28       138\n",
      "\n",
      "avg / total       0.86      0.89      0.86      1131\n",
      "\n",
      "[[978  15]\n",
      " [113  25]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building an Optimal Forest\n",
    "\n",
    "Of course we need to pick the best tree and there are many parameters to optimise. For a full list please refer to the documentation http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html.\n",
    "\n",
    "The main ones to focus on are:\n",
    "- number of trees\n",
    "- max_depth\n",
    "\n",
    "Here I will set up a grid of parameters to search through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running....\n",
      "Training time: 255.89262914657593s\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'n_estimators': np.arange(1,200,10),\n",
    "              'max_depth': np.arange(1,20,1), } \n",
    "\n",
    "clf_cv = GridSearchCV(ensemble.RandomForestClassifier(class_weight = 'balanced'), param_grid)\n",
    "\n",
    "print(\"Running....\")\n",
    "tic = time.time()\n",
    "clf_cv.fit(X_train, y_train)\n",
    "\n",
    "toc = time.time()\n",
    "print(\"Training time: {0}s\".format(toc - tic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the final (optimal) forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
      "            criterion='gini', max_depth=19, max_features='auto',\n",
      "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "            min_impurity_split=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=101, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "clf = clf_cv.best_estimator_\n",
    "print(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally check the accuracy on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.98      0.94       993\n",
      "          1       0.60      0.23      0.34       138\n",
      "\n",
      "avg / total       0.87      0.89      0.87      1131\n",
      "\n",
      "[[972  21]\n",
      " [106  32]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have better misclassification rate and f1-score than the above random forest model, although the training set size is smaller as cross valdition used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ExtraTreesClassifier\n",
    "\n",
    "Sklearn provides another class for classifying using forests: ExtraTreesClassifier. This class implements \"Extremely Randomised Forest\". As in random forests, a random subset of candidate features is used. **Additionally** thresholds are drawn at random for each candidate feature and the best of these randomly-generated thresholds is picked as the splitting rule. This reduces the variance of the model a even more but also increases bias.\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html\n",
    "\n",
    "## AdaBoost\n",
    "\n",
    "AdaBoost is available in the sklearn ensemble library. It is used in the same way as every other sklearn class. http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html\n",
    "\n",
    "The core principle is to fit a sequence of weak learners via boosting. Boosting is a process of increasing the weights of samples that were misclassified, then building a new classifier. The new classifier is thereby forced to concentrate on the examples that are missed by the previous ones in the sequence.\n",
    "\n",
    "By default AdaBoostClassifier uses DecisionTreeClassifier objects as the base classifier, however you can use a different classifier if you prefer. Check the docs for compatible classes.\n",
    "\n",
    "Some paramters to tune are:\n",
    "- n_estimators\n",
    "- learning_rate\n",
    "\n",
    "Below is an example of how to build an AdaBoost classifier. By default n_estimators = 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "          learning_rate=1.0, n_estimators=50, random_state=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = ensemble.AdaBoostClassifier()\n",
    "\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should notice that the training accuracy is even better than our single tree and even the forest!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.97      0.95       993\n",
      "          1       0.67      0.38      0.48       138\n",
      "\n",
      "avg / total       0.89      0.90      0.89      1131\n",
      "\n",
      "[[967  26]\n",
      " [ 86  52]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there are two available algorithms in AdaBoostClassifier: \"SAMME\" and \"SAMME.R\". The \"SAMME.R\" is the default algorithm used in Python and always return estimator weights as 1. \"SAMME\" will ouput unequal estimator weights (voting powers) for different estimators. Refer to the Python doc and following post for more information:\n",
    "\n",
    "https://stackoverflow.com/questions/31981453/why-estimator-weight-in-samme-r-adaboost-algorithm-is-set-to-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Disparate Classifiers\n",
    "\n",
    "We can go even further with ensemble classification. We can combine classes that are disparate and combine their predictions together for a more accurate classification. For example we could combine multiple RandomForests together or multiple Boosted Forests together!\n",
    "\n",
    "This is implemented in sklearn with the VotingClassifier class http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html.\n",
    "\n",
    "VotingClassifier provides two main parameters to specify:\n",
    "- voting scheme, how predictions from ensemble are combined\n",
    "- weights, we can weight each classifier's vote\n",
    "\n",
    "The voting scheme is either \"hard\" or \"soft\". Hard scheme means majority voting, while soft means we sum up the class probabilites of each classifer then make a decision.\n",
    "\n",
    "### Example\n",
    "\n",
    "Below I will use the iris dataset to demonstrate how using multiple classifiers together can slightly improve classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.90 (+/- 0.05) [Logistic Regression]\n",
      "Accuracy: 0.93 (+/- 0.05) [Random Forest]\n",
      "Accuracy: 0.91 (+/- 0.04) [Naive Bayes]\n",
      "Accuracy: 0.95 (+/- 0.05) [Ensemble]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data[:, 1:3], iris.target\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "clf1 = LogisticRegression(random_state=1)\n",
    "clf2 = RandomForestClassifier(random_state=1)\n",
    "clf3 = GaussianNB()\n",
    "\n",
    "eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='hard')\n",
    "\n",
    "for clf, label in zip([clf1, clf2, clf3, eclf], ['Logistic Regression', 'Random Forest', 'Naive Bayes', 'Ensemble']):\n",
    "    scores = cross_val_score(clf, X, y, cv=5, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-class Classification (and dealing with text, optional)\n",
    "\n",
    "In this section I want to introduce a more complex case. First the data contains more than two classes. Second the dataset contains only text data.\n",
    "\n",
    "    The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. The data is organized into 20 different newsgroups, each corresponding to a different topic. Some of the newsgroups are very closely related to each other (e.g. comp.sys.ibm.pc.hardware / comp.sys.mac.hardware), while others are highly unrelated (e.g misc.forsale / soc.religion.christian). Here is a list of the 20 newsgroups, partitioned (more or less) according to subject matter:\n",
    "*source: http://qwone.com/~jason/20Newsgroups/, https://archive.ics.uci.edu/ml/datasets/Twenty+Newsgroups**\n",
    "\n",
    "**The goal is:** given a text document, correctly assign it to the newsgroup from which it came from.\n",
    "\n",
    "We can directly use decision trees, forests etc for multi-class classification without any modification.\n",
    "\n",
    "The difficulty is transforming the text data into a numeric representation. Recall that a decision tree operates on features and threshold values for those features. Text does not satisify this requirement.\n",
    "\n",
    "### Bag of Words (Vectorising Text)\n",
    "\n",
    "A simple method of dealing with text is to treat each word in a corpus as a feature. For each document we count the number or frequency of each word.\n",
    "\n",
    "Below is a simple example. Note that X is a sparse matrix data type and some columns will be out of order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "corpus = ['This is the first document.',\n",
    "          'This is the second second document.']\n",
    "\n",
    "X = count_vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['document', 'first', 'is', 'second', 'the', 'this']\n"
     ]
    }
   ],
   "source": [
    "print(count_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 5)\t1\n",
      "  (1, 3)\t2\n",
      "  (1, 0)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 5)\t1\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In large datasets there will be lots of repeated words such as \"a\", \"is\" and \"the\" that don't carry much useful information. These terms should be ignored or given very small weights.\n",
    "\n",
    "We can use the tf–idf transform to boost the weights of uncommon words (which are likely domain specific) and shrink common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['document', 'first', 'is', 'second', 'the', 'this']\n",
      "  (0, 5)\t0.409090103683\n",
      "  (0, 2)\t0.409090103683\n",
      "  (0, 4)\t0.409090103683\n",
      "  (0, 1)\t0.574961866799\n",
      "  (0, 0)\t0.409090103683\n",
      "  (1, 5)\t0.289869335769\n",
      "  (1, 2)\t0.289869335769\n",
      "  (1, 4)\t0.289869335769\n",
      "  (1, 0)\t0.289869335769\n",
      "  (1, 3)\t0.814802474667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "tfidf = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(tfidf_vectorizer.get_feature_names())\n",
    "\n",
    "print(tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-grams\n",
    "\n",
    "Unfortunately this simple method has two major drawbacks:\n",
    "- it cannot handle phrases (multiple word expressions), which removes the order or dependancy information\n",
    "- it cannot handle typos\n",
    "\n",
    "So it is suggested that you use n-grams. Actually we have already been using unigrams so far. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['document', 'first', 'first document', 'is', 'is the', 'second', 'second document', 'second second', 'the', 'the first', 'the second', 'this', 'this is']\n",
      "  (0, 11)\t0.289569396523\n",
      "  (0, 3)\t0.289569396523\n",
      "  (0, 8)\t0.289569396523\n",
      "  (0, 1)\t0.406979683189\n",
      "  (0, 0)\t0.289569396523\n",
      "  (0, 12)\t0.289569396523\n",
      "  (0, 4)\t0.289569396523\n",
      "  (0, 9)\t0.406979683189\n",
      "  (0, 2)\t0.406979683189\n",
      "  (1, 11)\t0.224578375085\n",
      "  (1, 3)\t0.224578375085\n",
      "  (1, 8)\t0.224578375085\n",
      "  (1, 0)\t0.224578375085\n",
      "  (1, 12)\t0.224578375085\n",
      "  (1, 4)\t0.224578375085\n",
      "  (1, 5)\t0.631274140434\n",
      "  (1, 10)\t0.315637070217\n",
      "  (1, 7)\t0.315637070217\n",
      "  (1, 6)\t0.315637070217\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1, 2))\n",
    "\n",
    "tfidf = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(tfidf_vectorizer.get_feature_names())\n",
    "\n",
    "print(tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usenet Newsgroups\n",
    "\n",
    "Now we are armed with the right tools to tackle the problem at hand of classifying text documents.\n",
    "\n",
    "Let's transform the text documents to an n-gram representation and build a random forest to classify new documents.\n",
    "\n",
    "For the sake of running time I am going to manually pick the number of trees as 100 in my forest. Normally you should use CV to pick the best number of trees and the tree depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting and Transforming\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "categories = None\n",
    "remove = ('headers', 'footers', 'quotes')\n",
    "\n",
    "data_train = fetch_20newsgroups(subset='train', categories=categories,\n",
    "                                shuffle=True, random_state=42,\n",
    "                                remove=remove)\n",
    "\n",
    "data_test = fetch_20newsgroups(subset='test', categories=categories,\n",
    "                               shuffle=True, random_state=42,\n",
    "                               remove=remove)\n",
    "\n",
    "\n",
    "X_train = data_train.data\n",
    "y_train = data_train.target\n",
    "\n",
    "X_test = data_test.data\n",
    "y_test = data_test.target\n",
    "\n",
    "# tfidf_vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1, 2))\n",
    "tfidf_vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5, stop_words='english')\n",
    "\n",
    "print(\"Fitting and Transforming\")\n",
    "usenet_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training....\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "usenet_clf = ensemble.RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "print(\"Training....\")\n",
    "usenet_clf.fit(usenet_tfidf, y_train)\n",
    "print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.44      0.39      0.41       319\n",
      "          1       0.58      0.60      0.59       389\n",
      "          2       0.53      0.65      0.59       394\n",
      "          3       0.61      0.56      0.58       392\n",
      "          4       0.64      0.65      0.64       385\n",
      "          5       0.63      0.68      0.65       395\n",
      "          6       0.72      0.74      0.73       390\n",
      "          7       0.41      0.71      0.52       396\n",
      "          8       0.67      0.68      0.67       398\n",
      "          9       0.71      0.78      0.74       397\n",
      "         10       0.82      0.82      0.82       399\n",
      "         11       0.78      0.67      0.72       396\n",
      "         12       0.53      0.42      0.47       393\n",
      "         13       0.78      0.64      0.70       396\n",
      "         14       0.70      0.67      0.68       394\n",
      "         15       0.60      0.77      0.67       398\n",
      "         16       0.51      0.58      0.54       364\n",
      "         17       0.80      0.70      0.74       376\n",
      "         18       0.55      0.31      0.40       310\n",
      "         19       0.27      0.08      0.12       251\n",
      "\n",
      "avg / total       0.62      0.62      0.61      7532\n",
      "\n",
      "[[123   3   1   1   2   3   7  21   6   5   7   5   4   9  10  82   7  12\n",
      "    4   7]\n",
      " [  1 234  42   7   8  39   6  15   4   4   1   4   7   2  12   0   0   0\n",
      "    3   0]\n",
      " [  3  21 258  28  19  17   1  19   4   4   1   2   2   1   8   0   2   3\n",
      "    1   0]\n",
      " [  1  16  46 218  35  11  15  13   1   3   2   3  25   0   2   0   0   0\n",
      "    0   1]\n",
      " [  1   7  17  32 251   6  15  19   4   2   5   3  16   2   3   0   1   0\n",
      "    1   0]\n",
      " [  2  34  42  10   8 267   2  12   2   2   0   1   1   1   7   0   2   1\n",
      "    1   0]\n",
      " [  0   8   5  21  15   1 288  21   2   6   2   1   7   1   5   1   4   0\n",
      "    2   0]\n",
      " [  3   5   8   2   5   8   9 282  30   3   2   2  15   1   5   1   7   4\n",
      "    3   1]\n",
      " [  4   2   5   3   2   3   8  45 269  17   0   4   8   4   4   4   6   3\n",
      "    4   3]\n",
      " [  1   3   3   2   0   4   1  25   9 310  27   1   1   2   3   1   0   1\n",
      "    3   0]\n",
      " [  2   1   2   1   1   3   2  16   5  24 327   0   3   2   2   0   2   1\n",
      "    4   1]\n",
      " [  3  12   9   3   6   9   6  23   6   3   1 264  11   2   7   1  23   1\n",
      "    5   1]\n",
      " [  5  18  18  25  27  20  11  35  11   8   7  19 165   7   6   2   4   2\n",
      "    2   1]\n",
      " [  6  14   5   0   5   8  14  27  10  11   4   0   8 255   7   5   3   8\n",
      "    3   3]\n",
      " [  3  11   4   1   4   5   4  31   8   4   3   3  18   8 263   6   7   2\n",
      "    4   5]\n",
      " [ 17   2   7   0   0   2   3  17   5   2   1   0   5   1   6 307   1   7\n",
      "    3  12]\n",
      " [ 13   4   6   1   2   5   3  27  10   6   3  15   5   9   8   8 210   3\n",
      "   15  11]\n",
      " [ 26   0   1   0   1   5   1  12   9  10   1   5   3   3   3   7  12 262\n",
      "   13   2]\n",
      " [ 18   2   1   1   1   3   3  15   6   8   4   5   5  15  11   5  96   7\n",
      "   97   7]\n",
      " [ 47   4   3   1   3   3   0  18   3   6   1   3   1   4   3  84  28  11\n",
      "    8  20]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = usenet_clf.predict(tfidf_vectorizer.transform(X_test))\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
