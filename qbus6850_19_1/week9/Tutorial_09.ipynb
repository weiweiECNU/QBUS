{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QBUS6850: Tutorial 09 – Extreme Gradient Boosting\n",
    "\n",
    "# Objective:\n",
    "1. Get familiar with Gradient Boosting algorithm for regression and classification\n",
    "2. Learn how to use XGBoost package to do the classification and regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Gradient Boosting Classification \n",
    "In this task, let's review the example in Lecture 08.\n",
    "We will build a Gradient Boosting Decision Tree to do the classificatoin on Iris dataset.\n",
    "This dataset contains 150 data point, and each data is described with 4 features (namely sepal length & width(cm) and petal length & width (cm) )\n",
    "\n",
    "Please refer to QBUS6850_Lecture08.pdf for the detailed explanation.\n",
    "\n",
    "## Step 0: Import libs and data\n",
    "\n",
    "Read the code below, and try to answer the following questions:\n",
    "1. What is the __T__ in line 22?\n",
    "2. what is the purpose of __get_dummies__ in line 26?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import shuffle\n",
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "np.random.seed(5)\n",
    "\n",
    "# load the dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "t = iris.target\n",
    "\n",
    "# randomly shuffle the dataset\n",
    "N = len(t)\n",
    "index = list(range(N))\n",
    "shuffle(index)\n",
    "\n",
    "# select the first 12 data to train the model\n",
    "m = 12\n",
    "\n",
    "T = 3\n",
    "\n",
    "X1 = X[index[:m]]\n",
    "t1 = t[index[:m]]\n",
    "t1 = pd.get_dummies(t1)\n",
    "\n",
    "F = np.full((m,3),0.0)\n",
    "rho = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 1: Initial Models and Initial Probability (How from F to P?)\n",
    "\n",
    "Classification is also a regression problem with a specially defined loss function, such as the cross entropy over the data.\n",
    "\n",
    "Before you start reading the following code, please quickly review what is mathematical definition of cross entropy.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a new function for cal the probability\n",
    "def FtoP(F):\n",
    "    expF = np.exp(F)\n",
    "    a = expF.sum(axis=1)\n",
    "    P = expF / a[:,None]\n",
    "    return P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Calculate the negative gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "P = FtoP(F)\n",
    "NegG = t1.values - P\n",
    "print(NegG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Modeling from X to g(1) g(2) and g(3)\n",
    "\n",
    "Build a decision tree $h_1(x)$ of depth 1 from $X$ to $-g_1$, $-g_2$, and $-g_3$.\n",
    "\n",
    "For simplecity, we define max_depth =2 for each tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "baseH0 = []\n",
    "baseH1 = []\n",
    "baseH2 = []\n",
    "\n",
    "# First Round\n",
    "# for each t, build up a regressor\n",
    "for t in range(T):\n",
    "    regressor = DecisionTreeRegressor(random_state=0, max_depth=2)\n",
    "    h0 = regressor.fit(X1, NegG[:,0])\n",
    "    baseH0.append(h0)\n",
    "\n",
    "    regressor = DecisionTreeRegressor(random_state=0, max_depth=2)\n",
    "    h1 = regressor.fit(X1, NegG[:,1])\n",
    "    baseH1.append(h1)\n",
    "    \n",
    "    regressor = DecisionTreeRegressor(random_state=0, max_depth=2)\n",
    "    h2 = regressor.fit(X1, NegG[:,2])\n",
    "    baseH2.append(h2)\n",
    "    \n",
    "    #### not finished, need to update the F \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Updating $F:=F+\\rho h $ with $\\rho = 1$ for simplicity \n",
    "\n",
    "Please add the following statement in your for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    F[:,0] = F[:,0] + rho*h0.predict(X1)\n",
    "    F[:,1] = F[:,1] + rho*h1.predict(X1)\n",
    "    F[:,2] = F[:,2] + rho*h2.predict(X1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: F to P again\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    #Next Round\n",
    "    P = FtoP(F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Negative Gradient again\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    NegG = t1.values - P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Modeling Negative Gradients again, we have the second set of basic modeler $h_1(x),h_2(x)$, and $h_3(x)$: Decision trees of depth 1\n",
    "\n",
    "Note: the splitting points for this set of basic models are the same as the first set of basic models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now the models are stored in BaseH0, BaseH1 and BaseH2\n",
    "# Predict for a new case\n",
    "x = X[index[148:150]]\n",
    "F0 = 0.0\n",
    "F1 = 0.0\n",
    "F2 = 0.0\n",
    "\n",
    "for t in range(T):\n",
    "    F0 = F0 + baseH0[t].predict(x) \n",
    "    F1 = F1 + baseH1[t].predict(x)\n",
    "    F2 = F2 + baseH2[t].predict(x) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Final models \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "F = np.vstack((F0,F1,F2))\n",
    "F = F.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Prediction on new test data\n",
    "\n",
    "Finally, you should able to see the probability outputs.\n",
    "We will select the largest probability as the class label index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictedP = FtoP(F)\n",
    "print(predictedP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Installing the XGBoost package\n",
    "\n",
    "Open the Command Termial,\n",
    "\n",
    "If you are __python3__ user, then type in __\"pip3 install xgboost\"__ to install the XGBoost.\n",
    "\n",
    "If you are __python2__ user, then type in __\"pip install xgboost\"__ to install the XGBoost.\n",
    "\n",
    "\n",
    "\n",
    "For more information, please refer the official document at: https://xgboost.readthedocs.io/en/latest/build.html#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Predict Onset of Diabetes (Classification)\n",
    "\n",
    "This tutorial is based on https://machinelearningmastery.com/develop-first-xgboost-model-python-scikit-learn/\n",
    "\n",
    "In this task, we are going to use the Pima Indians onset of diabetes dataset.\n",
    "\n",
    "This dataset is comprised of 8 input variables that describe medical details of patients and one output variable to indicate whether the patient will have an onset of diabetes within 5 years.\n",
    "\n",
    "You can learn more about this dataset on the UCI Machine Learning Repository website.\n",
    "\n",
    "This is a good dataset for a first XGBoost model because all of the input variables are numeric and the problem is a simple binary classification problem. It is not necessarily a good problem for the XGBoost algorithm because it is a relatively small dataset and an easy problem to model.\n",
    "\n",
    "Download this dataset and place it into your current working directory with the file name “[pima-indians-diabetes.csv](https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv)”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1: Load and Prepare Data\n",
    "\n",
    "In this section we will load the data from file and prepare it for use for training and evaluating an XGBoost model.\n",
    "\n",
    "We will start off by importing the classes and functions we intend to use in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import loadtxt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can load the CSV file as a NumPy array using the NumPy function loadtext()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "dataset = loadtxt('pima-indians-diabetes.csv', delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must separate the columns (attributes or features) of the dataset into input patterns (X) and output patterns (Y). We can do this easily by specifying the column indices in the NumPy array format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split data into X and y\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we must split the X and Y data into a training and test dataset. The training set will be used to prepare the XGBoost model and the test set will be used to make new predictions, from which we can evaluate the performance of the model.\n",
    "\n",
    "For this we will use the train_test_split() function from the scikit-learn library. We also specify a seed for the random number generator so that we always get the same split of data each time this example is executed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split data into train and test sets\n",
    "seed = 7\n",
    "test_size = 0.33\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    Y, \n",
    "                                                    test_size=test_size, \n",
    "                                                    random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Train the XGBoost Model\n",
    "\n",
    "XGBoost provides a wrapper class to allow models to be treated like classifiers or regressors in the scikit-learn framework.\n",
    "\n",
    "This means we can use the full scikit-learn library with XGBoost models.\n",
    "\n",
    "The XGBoost model for classification is called XGBClassifier. We can create and and fit it to our training dataset. Models are fit using the scikit-learn API and the model.fit() function.\n",
    "\n",
    "Parameters for training the model can be passed to the model in the constructor. Here, we use the sensible defaults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit model no training data\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the parameters used in a trained model by printing the model, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can learn more about the defaults for the XGBClassifier and XGBRegressor classes in the [XGBoost Python scikit-learn API](https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn).\n",
    "\n",
    "You can learn more about the meaning of each parameter and how to configure them on the [XGBoost parameters page](https://xgboost.readthedocs.io/en/latest//parameter.html).\n",
    "\n",
    "We are now ready to use the trained model to make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Make Predictions with XGBoost Model\n",
    "\n",
    "We can make predictions using the fit model on the test dataset.\n",
    "\n",
    "To make predictions we use the scikit-learn function model.predict().\n",
    "\n",
    "By default, the predictions made by XGBoost are probabilities. Because this is a binary classification problem, each prediction is the probability of the input pattern belonging to the first class. We can easily convert them to binary class values by rounding them to 0 or 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make predictions for test data\n",
    "y_pred = model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have used the fit model to make predictions on new data, we can evaluate the performance of the predictions by comparing them to the expected values. For this we will use the built in accuracy_score() function in scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Boston Housing (regression)\n",
    "\n",
    "In this task, we are going to use the Boston housing dataset.\n",
    "\n",
    "This dataset contains information collected by the U.S Census Service concerning housing in the area of Boston Mass. It was obtained from the StatLib archive (http://lib.stat.cmu.edu/datasets/boston), and has been used extensively throughout the literature to benchmark algorithms. However, these comparisons were primarily done outside of Delve and are thus somewhat suspect. The dataset is small in size with only 506 cases.\n",
    "\n",
    "For more information of this dataset, please refer: https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1: Load and Prepare Data\n",
    "\n",
    "Similar to the previous task, we will load the data and prepare it for use for training and evaluating an XGBoost model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, train_test_split, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, mean_squared_error\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "rng = np.random.RandomState(31337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use __load_boston()__ function to load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Boston Housing: regression\")\n",
    "boston = load_boston()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the X and Y for your XGBoost model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = boston['target']\n",
    "X = boston['data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could also print out the dataset information by calling the __shape__ function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can see our dataset contains 506 datapoint, and each datapoint is described with 13 features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Applying k-fold Cross Validation\n",
    "\n",
    "Cross-validation is a statistical method used to estimate the skill of machine learning models.\n",
    "\n",
    "It is commonly used in applied machine learning to compare and select a model for a given predictive modeling problem in order to flag problems like overfitting and to give an insight on how the model will generalize to an independent dataset (i.e., an unknown dataset, for instance from a real problem)\n",
    "\n",
    "In this section, we will use k-fold cross-validation.\n",
    "In k-fold cross-validation, the original sample is randomly partitioned into k equal sized subsamples. \n",
    "Of the k subsamples, a single subsample is retained as the validation data for testing the model, and the remaining k − 1 subsamples are used as training data. \n",
    "The cross-validation process is then repeated k times, with each of the k subsamples used exactly once as the validation data. \n",
    "The k results can then be averaged to produce a single estimation. The advantage of this method over repeated random sub-sampling (see below) is that all observations are used for both training and validation, and each observation is used for validation exactly once. \n",
    "10-fold cross-validation is commonly used, but in general k remains an unfixed parameter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=2, shuffle=True, random_state=rng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For more information on KFold Cross Validation, please refer to: https://en.wikipedia.org/wiki/Cross-validation_(statistics)#k-fold_cross-validation\n",
    "\n",
    "You may also refer to the following link for the detailed information of each parameter and how to configure them by using scikit libs:\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Train and Make Prediction with XGBRegressor()\n",
    "\n",
    "We then call __split()__ function to generate indices to split data into training and test set.\n",
    "For each training and test set, we define a new __XGBRegressor()__ model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for train_index, test_index in kf.split(X):\n",
    "    xgb_model = xgb.XGBRegressor().fit(X[train_index], y[train_index])\n",
    "    predictions = xgb_model.predict(X[test_index])\n",
    "    actuals = y[test_index]\n",
    "    print(mean_squared_error(actuals, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Parameter Optimization\n",
    "\n",
    "Now we are able to use __XGBRegressor()__ function to make prediction.\n",
    "\n",
    "One remaining issue is about the value selection of the hyperparameters, i.e. learning rate alpha, number of classifiers (__n_estimators__), and maximun depth of each tree model (__max_depth__).\n",
    "\n",
    "By default, this function defines __learning_rate=0.1__, __n_estimators=100__, and __max_depth=3__.\n",
    "However, this settings may not suitable for our case.\n",
    "In order to find out what hyperparameter settings are suitable for our case, we use __GridSearchCV__ function to test different value of these hyperparameters.\n",
    "Note that, in this experiment, we only test different __max_depth__ and __n_estimators__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBRegressor()\n",
    "clf = GridSearchCV(xgb_model,\n",
    "                   {'max_depth': [2,4,6],\n",
    "                    'n_estimators': [50,100,200]}, verbose=1)\n",
    "clf.fit(X,y)\n",
    "print(clf.best_score_)\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
