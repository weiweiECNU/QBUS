{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>QBUS6850 - Machine Learning for Business</h1></center>\n",
    "\n",
    "# Tutorial 13 - Recommendation Systems\n",
    "\n",
    "## Surprise\n",
    "\n",
    "Surprise is a library for evaluating reccomendation algorithms. It can guide you in selecting the appropriate reccomendation algorithm for your particular task.\n",
    "\n",
    "It provides:\n",
    "- inbuilt sample datasets\n",
    "- baseline, nearest neighbour and factorisation algorithms\n",
    "- parameter optimisation tools (grid search)\n",
    "- evaluation tools\n",
    "\n",
    "Homepage: http://surpriselib.com\n",
    "\n",
    "Documentation: http://surprise.readthedocs.io/en/stable/\n",
    "\n",
    "### Installing Suprise\n",
    "\n",
    "Suprise is not available on the conda repository listing or conda-forge.\n",
    "\n",
    "Therefore we have to manually install it.\n",
    "\n",
    "1. Download Suprise v1.0.3 https://github.com/NicolasHug/Surprise/archive/v1.0.3.zip\n",
    "2. Unzip the folder\n",
    "3. Open Anaconda Prompt and navigate to the unzipped folder (use cd command to change directory) e.g.\n",
    "    - cd C:\\Users\\steve\\Downloads\\suprise\n",
    "4. Run the following commands in order:\n",
    "    - python setup.py build_ext -i\n",
    "    - python setup.py install\n",
    "5. Test Suprise by:\n",
    "    - Opening a Python session (just type \"python\" at the Anaconda Prompt or use Jupyter/Spyder)\n",
    "    - Importing a component of suprise e.g. \"from surprise import SVD\"\n",
    "    \n",
    "## Using Surprise\n",
    "\n",
    "In this example we will use Surprise's inbuilt MovieLens dataset. This dataset contains ratings of movies from users of the MovieLens website (http://movielens.org).\n",
    "\n",
    "We will use the [SVD](http://surprise.readthedocs.io/en/stable/matrix_factorization.html#surprise.prediction_algorithms.matrix_factorization.SVD) algorithm to predict all missing ratings (infill) from the dataset using the observed ratings.\n",
    "\n",
    "First let's load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from surprise import SVD\n",
    "from surprise import KNNBasic\n",
    "from surprise import BaselineOnly\n",
    "from surprise import Dataset\n",
    "from surprise import accuracy\n",
    "from surprise import GridSearch\n",
    "\n",
    "# Load the full dataset.\n",
    "data = Dataset.load_builtin('ml-100k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will do a grid search using cross validation to find the optimal parameters for the SVD method.\n",
    "\n",
    "To specify the number of cross validation folds to use in the evaluate() function you need to use the split() function of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.split(n_folds=3)\n",
    "\n",
    "param_grid = {'n_epochs': [5, 10], 'lr_all': [0.002, 0.005]}\n",
    "\n",
    "# Verbose = 2, this shows lots of useful output\n",
    "# http://surprise.readthedocs.io/en/stable/evaluate.html\n",
    "grid_search = GridSearch(SVD, param_grid, measures = ['RMSE'], verbose = 2)\n",
    "\n",
    "# Perform the grid search\n",
    "grid_search.evaluate(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print out the average RMSE for each fold and corresponding param pairs\n",
    "rmse_result = grid_search.cv_results['RMSE']\n",
    "param_pairs = grid_search.cv_results['params']\n",
    "\n",
    "for i in range(len(rmse_result)):\n",
    "    print(\"RMSE: {0}, Params: {1}\".format(rmse_result[i], param_pairs[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once our grid search is complete we can get the best model parameters by using the best_estimator attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pickup best model from grid search\n",
    "algo = grid_search.best_estimator['RMSE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can retrain our model on the full training set. In Suprise build_full_trainset() is every data point. It doesn't actually give you a train/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Retrain on full set\n",
    "trainset = data.build_full_trainset()\n",
    "algo.train(trainset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To predict a single item rating for a user/item pair you use the predict() function. The id of each user and item must be a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = algo.predict('374', '500')\n",
    "\n",
    "print(\"Prediction Object:\")\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Predicted Rating:\")\n",
    "pred[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a different prediction algorithm, e.g. the knn algorithm. Note that there is no optimization of k selection here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a desctiption of different similarities measurements\n",
    "# http://surprise.readthedocs.io/en/stable/similarities.html\n",
    "\n",
    "sim_options = {'name': 'cosine',\n",
    "               'user_based': False  # compute  similarities between items\n",
    "               }\n",
    "# http://surprise.readthedocs.io/en/stable/similarities.html\n",
    "#sim_options = {'name': 'pearson',\n",
    "#               ''user_based': True\n",
    "#               }\n",
    "\n",
    "algo_1 = KNNBasic(sim_options= sim_options)\n",
    "trainset = data.build_full_trainset()\n",
    "algo_1.train(trainset)\n",
    "\n",
    "pred = algo_1.predict('374', '500')\n",
    "\n",
    "print(\"Prediction Object:\")\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Predicted Rating:\")\n",
    "pred[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use the baseline appraoch. Note that there is no optimization of parameter selection here either."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bsl_options = {'method': 'als',\n",
    "               'n_epochs': 5,\n",
    "               'reg_u': 12,\n",
    "               'reg_i': 5\n",
    "               }\n",
    "algo_2 = BaselineOnly(bsl_options=bsl_options)\n",
    "\n",
    "trainset = data.build_full_trainset()\n",
    "algo_2.train(trainset)\n",
    "\n",
    "pred = algo_2.predict('374', '500')\n",
    "\n",
    "print(\"Prediction Object:\")\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Predicted Rating:\")\n",
    "pred[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting all missing entries\n",
    "\n",
    "First lets start by visualising our matrix of all observed entries.\n",
    "\n",
    "This matrix is quite sparse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_users = trainset.n_users\n",
    "n_items = trainset.n_items\n",
    "\n",
    "M = np.zeros((n_users, n_items))\n",
    "\n",
    "raw_ratings = data.raw_ratings\n",
    "\n",
    "for rating in raw_ratings:\n",
    "    r = int(rating[0]) - 1\n",
    "    c = int(rating[1]) - 1\n",
    "    v = rating[2]\n",
    "    \n",
    "    M[r, c] = v\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "fig = plt.figure()\n",
    "plt.imshow(M)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with our trained SVD algorithm we can create a list of all user/item pairs in our dataset to fill in the rating matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# What data did we end up with?\n",
    "\n",
    "rows, cols = np.meshgrid(np.linspace(0, n_users-1, n_users), np.linspace(0, n_items-1, n_items), indexing = 'ij')\n",
    "\n",
    "rows = np.ravel(rows)\n",
    "cols = np.ravel(cols)\n",
    "\n",
    "fullset = list()\n",
    "\n",
    "for i in range(n_users * n_items):\n",
    "    fullset.append( (str(int(rows[i])), str(int(cols[i])), 0) )\n",
    "    \n",
    "preds = algo.test(fullset)\n",
    "\n",
    "X = np.zeros((n_users, n_items))\n",
    "\n",
    "vals = [e[3] for e in preds]\n",
    "\n",
    "X[[int(r) for r in rows], [int(c) for c in cols]] = vals\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.imshow(X)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test split in Surprise\n",
    "\n",
    "To perform a train/test split in Surprise you could do something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = Dataset.load_builtin('ml-100k')\n",
    "raw_ratings = data.raw_ratings\n",
    "\n",
    "random.shuffle(raw_ratings)\n",
    "\n",
    "# A = 90% of the data, B = 10% of the data\n",
    "threshold = int(.9 * len(raw_ratings))\n",
    "A_raw_ratings = raw_ratings[:threshold]\n",
    "B_raw_ratings = raw_ratings[threshold:]\n",
    "\n",
    "data.raw_ratings = A_raw_ratings  # data is now the set A\n",
    "data.split(n_folds=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you can evaluate accuracy on the test set by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testset = data.construct_testset(B_raw_ratings)  # testset is now the set B\n",
    "# use the previously selected best model\n",
    "predictions = algo.test(testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the top 10 resutls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions[0:9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Low-Rank Matrix Completion (Optional)\n",
    "\n",
    "Unfortunately, Surprise is missing Low-Rank Matrix Completion (LRMC) from its set of algorithms.\n",
    "\n",
    "LRMC is incredibly powerful, despite its simplicity. It seeks to find a new matrix that is low-rank, while keeping the observed entries fixed.\n",
    "\n",
    "### Intuition\n",
    "\n",
    "Many users of netflix have similar or shared tastes. Their individual taste can be described as a combination of many other peoples tastes. Mathematically this means that each users rating vector (their taste) is a linear combination of other users. Therefore the total rating matrix must be relatively low-rank. So we should try to find the lowest rank matrix while keeping the existing user ratings fixed.\n",
    "\n",
    "You should notice that in the MovieLens example the final recovered matrix is low-rank (see last Figure).\n",
    "\n",
    "### Example\n",
    "\n",
    "Below we have a reference implementation and an example of usage on a synthetic dataset.\n",
    "\n",
    "First let's define a function to solve the LRMC and a helper function to solve the nuclear norm proximal problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse.linalg as sparse\n",
    "import scipy as sp\n",
    "\n",
    "def solve_nn(X, lam):\n",
    "    # Solves the proximal nuclear norm problem\n",
    "    # min lambda * |X|_* + 1/2*|X - Y|^2\n",
    "    # solved by singular value shrinking\n",
    "    \n",
    "    # Cai, J. F., Candès, E. J., & Shen, Z. (2010).\n",
    "    # A singular value thresholding algorithm for matrix completion.\n",
    "    # SIAM Journal on Optimization, 20(4), 1956-1982.\n",
    "    \n",
    "    U, s, V = sp.linalg.svd(X, full_matrices=False)\n",
    "    \n",
    "    new_s = np.maximum(s - lam, 0)\n",
    "    \n",
    "    return U.dot(np.diag(new_s)).dot(V), new_s\n",
    "    \n",
    "\n",
    "def solve_lrmc(M, omega, tau = 1, mu = 0.1, rho =1 , iterations = 100, tol = 10e-6):\n",
    "    # Solves the following problem\n",
    "    # min_A    tau * | A |_* \n",
    "    # s.t. P.*M = P.*A + P.*E\n",
    "\n",
    "    # which we convert to the unconstrained problem\n",
    "    # min_A   tau * | A |_* + < Y, P.*A - P.*M > + mu/2 | P.*A - P.*M |_F^2\n",
    "\n",
    "    # Adapted from\n",
    "    # Lin, Z., Liu, R., & Su, Z. (2011).\n",
    "    # Linearized alternating direction method with adaptive penalty for low-rank representation.\n",
    "    # In Advances in neural information processing systems (pp. 612-620).\n",
    "    \n",
    "    f_vals = np.zeros(iterations)\n",
    "    last_f_val = np.inf\n",
    "    \n",
    "    P = np.zeros(M.shape)\n",
    "    P[omega] = 1\n",
    "    \n",
    "    Y = np.zeros(M.shape)\n",
    "    A = np.zeros(M.shape)\n",
    "    \n",
    "    for k in range(iterations):\n",
    "        partial = mu * (P*A - (P*M - 1/mu * Y))\n",
    "        V = A - 1/rho * partial\n",
    "        \n",
    "        A, s = solve_nn(V, tau/rho)\n",
    "        \n",
    "        Y = Y + mu * (P*A - P*M)\n",
    "        \n",
    "        f_vals[k] = tau * np.sum(s)\n",
    "        \n",
    "        if (np.abs(f_vals[k] - last_f_val) <= tol):\n",
    "            break\n",
    "        else:\n",
    "            last_f_val = f_vals[k]\n",
    "        \n",
    "    return A, f_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to solve LRMC and a helped function to solve the nuclear norm proximal problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then create some synthetic low-rank data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 200 x 100  = 200 x 5 by 5 x 100\n",
    "m = 200\n",
    "n = 100\n",
    "r = 5\n",
    "\n",
    "A = np.dot(np.random.rand(m, r), np.random.rand(r, n))\n",
    "\n",
    "print(np.linalg.matrix_rank(A))\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.imshow(A)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then randomly sample from the data. $\\mathbf M$ is our partially observed ratings matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_prop = 0.3\n",
    "\n",
    "omega_linear = np.random.choice(int(m*n), int(m*n*sample_prop))\n",
    "\n",
    "omega  = np.unravel_index(omega_linear, (m, n))\n",
    "\n",
    "M = np.zeros(A.shape)\n",
    "M[omega] = A[omega]\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.imshow(M)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally solve the LRMC objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Solving...\")\n",
    "\n",
    "X, fvals = solve_lrmc(M, omega)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, sharey=True)\n",
    "ax1.imshow(A)\n",
    "ax1.set_title('Original')\n",
    "ax2.imshow(M)\n",
    "ax2.set_title(\"Observed\")\n",
    "ax3.imshow(X)\n",
    "ax3.set_title(\"Recovered\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the first row of $\\mathbf A$, $\\mathbf M$ and $\\mathbf X$.\n",
    "\n",
    "Note that $\\mathbf A$ and $\\mathbf X$ are very close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(A[0,:])\n",
    "\n",
    "print(M[0,:])\n",
    "\n",
    "print(X[0,:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
