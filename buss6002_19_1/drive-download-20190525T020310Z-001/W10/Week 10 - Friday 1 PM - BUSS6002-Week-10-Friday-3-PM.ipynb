{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BUSS6002 Week 10: MAP REDUCE\n",
    "\n",
    "Project notes from class: https://www.evernote.com/shard/s175/sh/a49f29a6-8e46-4f42-b801-fa446f41f380/41d5809bb5c8df3efd9ed8492c09133d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# RE: Regular Expressions (Regex)\n",
    "# This helps us grab parts of a string by specifying a rule ('expression')\n",
    "import re\n",
    "\n",
    "# This is used to run the MAP REDUCE job\n",
    "# It will allocate and distribute jobs to different workers automatically\n",
    "# You have to write the map and reduce steps yourself\n",
    "from mockr import run_stream_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hello', 1), ('this', 1), ('is', 2), ('a', 1), ('sample', 1), ('string', 1), ('it', 1), ('very', 1), ('simple', 1), ('goodbye', 1)]\n"
     ]
    }
   ],
   "source": [
    "# This regular expression matches words\n",
    "# Test it at https://www.regexpal.com\n",
    "# From the website we can see that the expression below will help us search for all words in a sentence (and words include ')\n",
    "WORD_RE = re.compile(r\"[\\w']+\")  # This is now an object that will find words for us\n",
    "\n",
    "# In this case, we will be splitting our string up into chunks of lines.\n",
    "# Example: \"Hello!\\nThis is a sample string.\\nIt is very simple.\\nGoodbye!\" will become\n",
    "#   Chunk 1: \"Hello!\"\n",
    "#   Chunk 2: \"This is a sample string.\"\n",
    "#   Chunk 3: \"It is very simple.\"\n",
    "#   Chunk 4: \"Goodbye!\"\n",
    "# Example: WORD_RE.findall(\"This is a sample string.\")\n",
    "#   This will give you [\"This\", \"is\", \"a\", \"sample\", \"string\"]\n",
    "def map_fn(chunk):\n",
    "    # Use the regex to find all words in each chunk\n",
    "    # The chunk is a line of text because we are\n",
    "    # using run_stream_job\n",
    "    for word in WORD_RE.findall(chunk):\n",
    "        # Emit a result using the word as the key and\n",
    "        # the number of times it occured. We emit once\n",
    "        # for each word so this value is 1.\n",
    "        yield (word.lower(), 1)\n",
    "\n",
    "def reduce_fn(key, values):\n",
    "    # Recieves all the values for each key (unique word)\n",
    "    # then sums them together for the total count\n",
    "    yield (key, sum(values))\n",
    "\n",
    "# \"\\n\" is the newline character which seperates the lines of text\n",
    "input_str = \"Hello!\\nThis is a sample string.\\nIt is very simple.\\nGoodbye!\"\n",
    "\n",
    "# run_stream_job expects a newline delimited string, map function and reduce function\n",
    "# and returns a list of results\n",
    "# https://mockr.readthedocs.io/en/latest/api.html#mockrmockmr.run_stream_job\n",
    "results = run_stream_job(input_str, map_fn, reduce_fn)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Break down the process above one by one\n",
    "\n",
    "- This part is not necessary for actually getting the MAP REDUCE job done.\n",
    "- This is for your understanding only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we want to break the string into separate lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello!', 'This is a sample string.', 'It is very simple.', 'Goodbye!']\n"
     ]
    }
   ],
   "source": [
    "input_str = \"Hello!\\nThis is a sample string.\\nIt is very simple.\\nGoodbye!\"\n",
    "\n",
    "# Split the string into lines and store in a list\n",
    "lines_of_text = input_str.split(\"\\n\")\n",
    "\n",
    "print(lines_of_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will do the map job:\n",
    "\n",
    "- First, each line will be split into words.\n",
    "    - e.g. 'This is a sample string.' will become ['This', 'is', 'a', 'sample', 'string']\n",
    "- Then, we will collect the result of the map jobs into a big list.\n",
    "\n",
    "For example, we can have 4 chunks:\n",
    "- Chunk 1: \"Hello!\" -> ['Hello'] -> [('hello', 1)]\n",
    "- Chunk 2: \"This is a sample string.\" -> ['This', 'is', 'a', 'sample', 'string'] -> [('this', 1), ('is', 1), ('a', 1), ('sample', 1), ('string', 1)]\n",
    "- Chunk 3: \"It is very simple.\" -> ['It', 'is', 'very', 'simple'] -> [('it', 1), ('is', 1), ('very', 1), ('simple', 1)]\n",
    "- Chunk 4: \"Goodbye!\" -> ['Goodbye'] -> [('goodbye', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('hello', 1)], [('this', 1), ('is', 1), ('a', 1), ('sample', 1), ('string', 1)], [('it', 1), ('is', 1), ('very', 1), ('simple', 1)], [('goodbye', 1)]]\n"
     ]
    }
   ],
   "source": [
    "# We will store the output of map_fn in here\n",
    "word_count_lists = []\n",
    "\n",
    "# For every line of text\n",
    "for line in lines_of_text:\n",
    "    # Apply the map function (split and count words)\n",
    "    # Save the result as a list in our list\n",
    "    word_count_lists.append(list(map_fn(line)))\n",
    "\n",
    "# Show the result of mapping\n",
    "print(word_count_lists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the lists are still separated by chunks. We now need to reorganize and put all the individual key-value pairs (word, count) into one big list.\n",
    "\n",
    "This would be the beginning and preparation of the reduce job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hello', 1), ('this', 1), ('is', 1), ('a', 1), ('sample', 1), ('string', 1), ('it', 1), ('is', 1), ('very', 1), ('simple', 1), ('goodbye', 1)]\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "# word_count_lists is a list of lists\n",
    "# Flatten the list of words to make it simpler by chaining lists together\n",
    "word_count_list_flat = list(itertools.chain.from_iterable(word_count_lists))\n",
    "\n",
    "print(word_count_list_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to put the data into the right reduce worker.\n",
    "\n",
    "That is, the data with the same keys should be going to the same reduce worker and stored in the same processor / machine / worker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello: [('hello', 1)]\n",
      "this: [('this', 1)]\n",
      "is: [('is', 1), ('is', 1)]\n",
      "a: [('a', 1)]\n",
      "sample: [('sample', 1)]\n",
      "string: [('string', 1)]\n",
      "it: [('it', 1)]\n",
      "very: [('very', 1)]\n",
      "simple: [('simple', 1)]\n",
      "goodbye: [('goodbye', 1)]\n"
     ]
    }
   ],
   "source": [
    "# SHUFFLE/SORT STAGE\n",
    "from collections import defaultdict\n",
    "\n",
    "# Create a dictionary where the default value is a list\n",
    "word_tuple_dict = defaultdict(list)\n",
    "\n",
    "for kv_pair in word_count_list_flat:\n",
    "    # For each unique key append the (word, count) tuple to that keys list\n",
    "    # kv_pair[0] specifies the reduce machine's name using the unique word as the key\n",
    "    # In that machine, you will put in (append) the relevant data (kv_pair)\n",
    "    word_tuple_dict[kv_pair[0]].append(kv_pair)\n",
    "\n",
    "# Print it in a nice format:\n",
    "for k, v in word_tuple_dict.items():\n",
    "    print(str(k) +\": \" + str(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will perform the actual reduce job, which is to sum all the values in that worker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('hello', 1)], [('this', 1)], [('is', 2)], [('a', 1)], [('sample', 1)], [('string', 1)], [('it', 1)], [('very', 1)], [('simple', 1)], [('goodbye', 1)]]\n"
     ]
    }
   ],
   "source": [
    "# REDUCE STAGE\n",
    "results = []\n",
    "\n",
    "# Here, the key value pair is the REDUCE machine, not the data\n",
    "# e.g. k = 'hello', v = [('hello', 1)]\n",
    "for k, v in word_tuple_dict.items():\n",
    "    # Get the counts from the list of k/v pairs\n",
    "    # For example, if v = [('hello', 1)], then\n",
    "    #    t = ('hello', 1)\n",
    "    #    t[1] = 1\n",
    "    # So the list will just become [1]\n",
    "    # For example, if v = [('is', 1), ('is', 1)], then\n",
    "    #    FIRST TIME: t = ('is', 1), SECOND TIME: t = ('is', 1)\n",
    "    #    FIRST TIME: t[1] = 1, SECOND TIME: t[1] = 1\n",
    "    # So the list will just become [1, 1]\n",
    "    vals_list = [t[1] for t in v]\n",
    "    \n",
    "    # Apply the reduce_fn to the word and counts pair\n",
    "    # reduce_fn will yield a (key, value) tuple\n",
    "    # inside a generator object which we convert to a list\n",
    "    # For example, [1] will add up to 1\n",
    "    # For example, [1, 1] will add up to 2\n",
    "    results.append(list(reduce_fn(k, vals_list)))\n",
    "    \n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hello', 1), ('this', 1), ('is', 2), ('a', 1), ('sample', 1), ('string', 1), ('it', 1), ('very', 1), ('simple', 1), ('goodbye', 1)]\n"
     ]
    }
   ],
   "source": [
    "# Flatten the results to make them more readable\n",
    "results_flat = list(itertools.chain.from_iterable(results))\n",
    "\n",
    "print(results_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression with Map Reduce\n",
    "\n",
    "- Linear Regression with Map Reduce gives you EXACT answers\n",
    "- So, you achieve speed up for large datasets without compromising accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def map_linear_fn(chunk):\n",
    "    # Get the dependant variable y\n",
    "    y = chunk['Price'].values\n",
    "\n",
    "    # Get the independant/feature variables\n",
    "    # which is everything except the price column\n",
    "    X_vals = chunk[chunk.columns.difference(['Price'])].values\n",
    "\n",
    "    # Get the number of data points\n",
    "    m = chunk.shape[0]\n",
    "\n",
    "    # Insert a column of \"1\"s for the intercept term \n",
    "    X = np.column_stack((np.ones(m), X_vals))\n",
    "\n",
    "    # Convert to matrix to make multiplication easier\n",
    "    X = np.asmatrix(X)\n",
    "\n",
    "    # Calculate required multiplications\n",
    "    XtX = X.T*X\n",
    "    Xty = X.T * y.reshape(m,1)\n",
    "\n",
    "    # Yield the result\n",
    "    yield(\"result\", [XtX, Xty])\n",
    "\n",
    "def reduce_linear_fn(key, values):\n",
    "\n",
    "    # Create lists to accumulate the matrices/vectors in\n",
    "    XtX_list = []\n",
    "    Xty_list = []\n",
    "\n",
    "    # Combine all the results from all workers into a big list\n",
    "    for result_list in values:\n",
    "        XtX_list.append(result_list[0])\n",
    "        Xty_list.append(result_list[1])\n",
    "\n",
    "    # Sum up all the XtX matrices\n",
    "    XtX = np.asmatrix(sum(XtX_list))\n",
    "\n",
    "    # Sum up all the Xty vectors\n",
    "    Xty = sum(Xty_list)\n",
    "\n",
    "    # Solve the linear regression objective\n",
    "    betas = np.linalg.inv(XtX) * Xty\n",
    "\n",
    "    yield (key, betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('result', matrix([[-4.54226376e+04],\n",
      "        [-4.32338317e+02],\n",
      "        [ 3.99294120e+04],\n",
      "        [-2.60221781e+04],\n",
      "        [-2.15419771e+01],\n",
      "        [-1.92386741e+03],\n",
      "        [ 7.56350149e+03],\n",
      "        [-1.89659638e+03],\n",
      "        [ 8.57720047e+01],\n",
      "        [ 5.81019654e+04]]))]\n"
     ]
    }
   ],
   "source": [
    "from mockr import run_pandas_job\n",
    "\n",
    "# Load in your data\n",
    "df = pd.read_excel(\"BatonRouge.xls\")\n",
    "\n",
    "# Get rid of any columns that are not numerical\n",
    "df = df[df.columns.difference(['Style'])]\n",
    "\n",
    "# Run the mocker job on pandas dataframes\n",
    "results = run_pandas_job(df, map_linear_fn, reduce_linear_fn, n_chunks = 4)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-4.54226376e+04 -4.32338317e+02  3.99294120e+04 -2.60221781e+04\n",
      " -2.15419771e+01 -1.92386741e+03  7.56350149e+03 -1.89659638e+03\n",
      "  8.57720047e+01  5.81019654e+04]\n"
     ]
    }
   ],
   "source": [
    "# Simplify the result to make it more readable\n",
    "params_mr = np.array(results[0][1]).ravel()\n",
    "\n",
    "print(params_mr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-4.54226376e+04 -4.32338317e+02  3.99294120e+04 -2.60221781e+04\n",
      " -2.15419771e+01 -1.92386741e+03  7.56350149e+03 -1.89659638e+03\n",
      "  8.57720047e+01  5.81019654e+04]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr_obj = LinearRegression()\n",
    "\n",
    "features = df[df.columns.difference(['Price'])]\n",
    "target = df['Price']\n",
    "\n",
    "lr_obj.fit(features, target)\n",
    "\n",
    "params_sk = np.append(np.array(lr_obj.intercept_), lr_obj.coef_)\n",
    "\n",
    "print(params_sk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
