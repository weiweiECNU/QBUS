{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# BUSS6002 Week 11 Python Material"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Import and setup\n",
    "%pylab notebook\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "py.init_notebook_mode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Python Interlude I: Rules Based Text Analytics\n",
    "Here, we'll cover the practical aspects of text analytics that *don't* involve statistics, machine learning or models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Texta Analysis in SQL is possible, but not pretty\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth',160)\n",
    "con = sqlite3.connect('data/kaggle_airline_twitter.sqlite')\n",
    "my_query = 'SELECT tweet_id, name, text FROM Tweets WHERE text LIKE \"%wait%\" OR text like \"%delay%\" OR text like \"%late%\"'\n",
    "df_tweets_sql = pd.read_sql_query(my_query, con)\n",
    "print(\"Found {} matching tweets\".format(len(df_tweets_sql)))\n",
    "df_tweets_sql.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Very simple text search in python\n",
    "- We can treat a dataframe column as a string by using \"str\", and use the \"contains\" function. This gives us a true/false vector indicating which rows have a match in that column.\n",
    "- Using \"loc\", we can select only those rows which are \"True\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Search for the word \"bags\" in any tweet.\n",
    "matches_bag = df_tweets_sql.text.str.contains('bags')\n",
    "df_tweets_sql.loc[matches_bag, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## A better way for text analysis: Python's NLTK package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "snowball_stemmer = EnglishStemmer()\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "\n",
    "words = pd.Series([\"wait\", \"waiter\", \"waited\", \"delay\", \"delays\", \"delayed\", \"delaying\", \"late\", \"later\"])\n",
    "\n",
    "pd.DataFrame({\"Original\": words,\n",
    "              \"Porter\": words.apply(porter_stemmer.stem),\n",
    "              \"Snowball (Porter 2)\": words.apply(snowball_stemmer.stem),\n",
    "              \"Lancaster\": words.apply(lancaster_stemmer.stem),\n",
    "             })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Repeat the \"delayed flight\" search, with NLTK\n",
    "- First stem all the words in the tweets, then search for stemmed versions of the words used in the SQL query.\n",
    "- Then merge the sql search approach with the stemmed approach, and identify where different decisions were made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Get the whole table as a pandas dataframe.\n",
    "con = sqlite3.connect('data/kaggle_airline_twitter.sqlite')\n",
    "df_tweets = pd.read_sql_query('SELECT tweet_id, tweet_created, name, text FROM Tweets', con) \n",
    "\n",
    "def stem_tweet(tweet):\n",
    "    \"\"\"Break up a sentence, stem each word, and put it back together.\"\"\"\n",
    "    words = nltk.word_tokenize(tweet)\n",
    "    stemmed_tweet = ' '.join([snowball_stemmer.stem(word) for word in words])\n",
    "    return stemmed_tweet \n",
    "    \n",
    "df_tweets['stemmed_text'] = df_tweets.text.apply(stem_tweet)\n",
    "df_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Stem each search term, then run the search.\n",
    "search_terms = ['wait', 'delay', 'late']\n",
    "search_terms = [snowball_stemmer.stem(term) for term in search_terms]\n",
    "df_tweets['is_match'] = df_tweets.stemmed_text.str.contains('|'.join(search_terms))\n",
    "print(df_tweets.is_match.sum())\n",
    "df_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Stemming vs Simple Search\n",
    "- The simple search would match words like \"chocolate\" for search term \"late\", whereas stemming does not.\n",
    "- But we discover an awkward, but very commonly occuring problem: The data set has been tampered with, but presented as clean / raw data (clearly every instance of \"late\" has been replaced by \"Late Flight\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df_tweets_merged = (pd\n",
    "                    .merge(df_tweets, df_tweets_sql, how='left', on='tweet_id')\n",
    "                    .assign(is_match_sql = lambda d: pd.notnull(d.name_y))\n",
    "                    .filter(['text_x', 'is_match', 'is_match_sql'])\n",
    "                   )\n",
    "display(df_tweets_merged.query('is_match != is_match_sql').head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Rules Based Search III: Regex\n",
    "- Can represent highly complex deterministic queries.\n",
    "- Is very difficult to learn/interpret (easiest if you can see results in real time, e.g. http://www.regexr.com/)\n",
    "\n",
    "      '.*[ ]happy.*' # Simple regex searching for \"happy\" preceeded by a space (to avoid \"unhappy\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Example: Functional date validator in format dd/mm/yyyy, dd-mm-yyyy or dd.mm.yyyy. It allows leading zeros but does not require them.\n",
    "      \n",
    "      ^(?:(?:31(\\/|-|\\.)(?:0?[13578]|1[02]))\\1|(?:(?:29|30)(\\/|-|\\.)(?:0?[1,3-9]|1[0-2])\\2))(?:(?:1[6-9]|[2-9]\\d)?\\d{2})$|^(?:29(\\/|-|\\.)0?2\\3(?:(?:(?:1[6-9]|[2-9]\\d)?(?:0[48]|[2468][048]|[13579][26])|(?:(?:16|[2468][048]|[3579][26])00))))$|^(?:0?[1-9]|1\\d|2[0-8])(\\/|-|\\.)(?:(?:0?[1-9])|(?:1[0-2]))\\4(?:(?:1[6-9]|[2-9]\\d)?\\d{2})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "regexp = '.*[ ]happy.*'\n",
    "df_tweets.loc[df_tweets.text.str.contains(regexp), ('name', 'text')].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "regexp_1 = '^(0?[1-9]|[12][0-9]|3[01])[\\/\\-](0?[1-9]|1[012])[\\/\\-]\\d{4}$'\n",
    "possible_dates = pd.Series(['01/01/2015', '1/1/2000', 'raspberry', '15-05-2017', '30/02/1988', '31/02/1988'])\n",
    "(possible_dates\n",
    " .to_frame()\n",
    " .assign(regexp_1 = possible_dates.str.contains(regexp_1))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "regexp_2 = '(^(((0[1-9]|1[0-9]|2[0-8])[\\/](0[1-9]|1[012]))|((29|30|31)[\\/](0[13578]|1[02]))|((29|30)[\\/](0[4,6,9]|11)))[\\/](19|[2-9][0-9])\\d\\d$)|(^29[\\/]02[\\/](19|[2-9][0-9])(00|04|08|12|16|20|24|28|32|36|40|44|48|52|56|60|64|68|72|76|80|84|88|92|96)$)'\n",
    "(possible_dates\n",
    " .to_frame()\n",
    " .assign(regexp_1 = possible_dates.str.contains(regexp_1), regexp_2 = possible_dates.str.contains(regexp_2))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Rules Based Text Analysis: Sentiment Analysis\n",
    "> \"VADER (Valence Aware Dictionary and sEntiment Reasoner) is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media.\"\n",
    "\n",
    "- Be very careful with rules based sentiment (typically assigning positive/negative points for each word, and adding them up). If a meal in a restaurant is \"to die for\", that's a good thing!\n",
    "- VADER is specifically tuned for Twitter (which is a very distinctive data set), so we're pretty safe here...\n",
    "- VADER has approx 600 lines of code, and a \"lexicon\" (dictionary) of over 7,000 terms.\n",
    "\n",
    "https://github.com/cjhutto/vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sent = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Assign polarity scores to measure sentiment intensity\n",
    "df_tweets = (df_tweets\n",
    " .assign(sentiment=df_tweets.text.apply(lambda s: sent.polarity_scores(s)['compound']))\n",
    " .sort_values('sentiment', ascending=False)\n",
    ")\n",
    "df_tweets.filter(['text', 'sentiment']).iloc[::1000,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Model Based Text Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Supervised N-Gram Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## Split into training and test set by dates\n",
    "\n",
    "figure(figsize=(10,3))\n",
    "df_tweets['date'] = pd.to_datetime(df_tweets.tweet_created)\n",
    "df_tweets_train = df_tweets[df_tweets.date < '2015-02-23']\n",
    "df_tweets_test = df_tweets[df_tweets.date >= '2015-02-23']\n",
    "df_tweets_train['date'].hist(bins=15)\n",
    "df_tweets_test['date'].hist(bins=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline                         \n",
    "#Pipeline of transforms with a final estimator\n",
    "from sklearn.feature_extraction.text import CountVectorizer   \n",
    "# Convert a collection of text documents to a matrix of token counts\n",
    "from sklearn.feature_extraction.text import TfidfTransformer  \n",
    "# Transform count matrix to a normalized tf-idf representation\n",
    "from sklearn.linear_model import SGDClassifier                \n",
    "# regularized linear models with stochastic gradient descent \n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier()),\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    'vect__max_df': (0.5, 0.75, 1.0), \n",
    "    'vect__max_features': (None, 5000, 10000, 50000),\n",
    "    'vect__ngram_range': ((1, 1),),  # unigrams or bigrams\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__alpha': (0.00001, 0.000001),\n",
    "    'clf__penalty': ('l2', 'elasticnet'),\n",
    "    'clf__n_iter': (10, 50, 80),\n",
    "}\n",
    "\n",
    "pipeline_search = RandomizedSearchCV(pipeline, \n",
    "                                     parameters, \n",
    "                                     n_iter =10, \n",
    "                                     scoring='roc_auc', \n",
    "                                     n_jobs=-1, \n",
    "                                     verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Assume that sentiment classifier above is the true label\n",
    "pipeline_search.fit(df_tweets_train.text, df_tweets_train['sentiment'] > 0.5)\n",
    "sent_pred_train = pipeline_search.predict(df_tweets_train.text)\n",
    "sent_pred_test = pipeline_search.predict(df_tweets_test.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "sent_test = df_tweets_test['sentiment'] > 0.5\n",
    "print(\"Test set classification report\")\n",
    "print(metrics.classification_report(sent_test, sent_pred_test))\n",
    "\n",
    "print(\"Test set confusion matrix\")\n",
    "print(metrics.confusion_matrix(sent_test, sent_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "vect = pipeline_search.best_estimator_.steps[0][-1]\n",
    "vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "clf = pipeline_search.best_estimator_.steps[2][-1]\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Investigate results\n",
    "\n",
    "df_vocab = (pd.DataFrame([{'feature_index': v, 'term': k} for k, v in vect.vocabulary_.items()])\n",
    "            .set_index('feature_index')\n",
    "            .sort_index()\n",
    "            .assign(coefficient = clf.coef_.flatten(), )\n",
    "            .sort_values('coefficient')\n",
    "           )\n",
    "df_vocab['ngram'] = df_vocab.term.apply(lambda s: len(s.split(' ')))\n",
    "display(df_vocab.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "display(df_vocab.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vocab.sample(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Word Embedding\n",
    "- In n-gram models we counted the number of occurrences of a word (n-gram) and encoded the feature with a random number.\n",
    "- Word embedding aims to represent meaning with the assigned numbers.\n",
    "\n",
    "> What if we could assign words a feature number (or vector of numbers), where similar words got similar numbers?\n",
    "\n",
    "**Word2Vec** takes a text corpus as input and produces the word vectors as output. \n",
    "\n",
    "It first constructs a vocabulary from the training text data and then learns vector representation of words.\n",
    "The resulting word vector file can be used as features in many natural language processing and machine learning applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> What if we could do maths with those vectors? \n",
    "> e.g King - Man + Woman = Queen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Train a model to create word vectors with deep learning \n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "wv_model = Word2Vec(sentences=df_tweets.text.apply(lambda s: s.split(' ')))\n",
    "wv_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Investigate the learned representations by finding the closest words for a user-specified word. \n",
    "\n",
    "df_words = pd.DataFrame({'word_a': ['delta', 'amazing', 'ridiculous', 'transfer', 'transfer', 'reimburse', 'reimburse'],\n",
    "                         'word_b': ['united', 'wonderful', 'horrible', 'plane', 'change', 'claim', 'dog']})\n",
    "\n",
    "df_words['similarity'] = df_words.apply(lambda r: wv_model.similarity(r.word_a, r.word_b), axis=1)\n",
    "df_words['word_a_vec'] = df_words.word_a.apply(lambda w: wv_model[w])\n",
    "df_words['word_b_vec'] = df_words.word_b.apply(lambda w: wv_model[w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results\n",
    "\n",
    "df_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "livereveal": {
   "height": 720,
   "scroll": true,
   "start_slideshow_at": "selected",
   "theme": "simple",
   "transition": "zoom",
   "width": 1280
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
